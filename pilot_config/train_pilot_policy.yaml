
hydra:
  run:
    dir: ${log.project_folder}/${log.run_name}/configs

defaults:
  - _self_
  - linear_encoder: tcn       # mlp | tcn | ..
  - vision_encoder: efficientnet # depth_feature_extractor | efficientnet
  - policy_model: pidiff       # pidiff | vint | .. 
  - datasets: asher # all | go_to_the_picture
  # - log: defaults


# device setup
device: cuda

# training setup
training:
  # goal_condition: True
  batch_size: 16
  eval_batch_size: 16
  epochs: 30
  current_epoch: 0
  gpu_ids: [0]
  num_workers: 12
  lr: 1e-4 # 5e-4 5e-5 1e-5 1e-4  
  optimizer: adamw
  clipping: True # Clip gradients
  clipping_max_norm: 1.
  scheduler: cosine  # cyclic | cosine | plateau
  warmup: False 
  warmup_epochs: 4
  cyclic_period: 10
  plateau_patience: 3
  plateau_factor: 0.5
  seed: 42
  use_ema: False # not in use
  goal_mask_prob: 0 # 0 for no mask att all -> task specific
                      # 1 for mask all -> task agnostic, only explore
                      # p ~ (0,1) -> task specific with some explorations capabilities
  modal_dropout_prob: 0 # 0 for no no dropout , 1 for dropout one modal anytime


# data config
data:
  goal_condition: True
  
  # normalization for the action space
  # normalization in the dataset class at data/pilot_dataset.py
  normalize: True
  # context
  context_type: temporal
  target_observation_enable: True
  target_context: True
  context_size: 4
  distance:
    min_dist_cat: 0
    max_dist_cat: 12
  action:
    min_dist_cat: 3
    max_dist_cat: 10
  # distance threshold used to seperate the close and the far subgoals that are sampled per datapoint
  # action output params
  # len_traj_pred: 8 # TODO: chang to pred_horizon
  pred_horizon: 8 # TODO: chang to pred_horizon
  action_horizon: 6
  learn_angle: True
  # dataset specific parameters
  image_size: [96, 96] # width, height
  # image_size: [85, 64] # width, height
  goal_type: "image"
  obs_type: "image"
  img_type: "depth" # if obs_type = image->  depth (1) | rgb (3) 
                    # do not forget to change the in_channels param in encoder_model as well

# logging config
log:
  project_folder: /home/roblab20/dev/pilot/pilot_bc/pilot_train/logs
  # project_folder: /home/nimrod/dev/pilot/pilot_bc/pilot_train/logs

  run_name: train_pilot_policy/${policy_model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  ## =0 turns off
  num_images_log: 4
  print_log_freq: 100 # in iterations
  eval_log_freq: 3  # in iterations
  image_log_freq: 1 # # in iterations
  eval_fraction: 0.25
  eval_freq: 1 # in epochs
  save_model_freq: 10
  wandb: 
    run: 
      enable: true
      name: ${log.run_name} ## modify
      log_freq: 10 # in iterations
    setup:
      project: pilot-release
      entity: nimrodcurtis
      mode: online

####### was ##########

# # model params
# model_type: vint
# obs_encoder: "efficientnet-b0" # by default, this is imagenet pretrained
# obs_encoding_size: 512
# mha_num_attention_heads: 4
# mha_num_attention_layers: 4
# mha_ff_dim_factor: 4
# late_fusion: False

# normalization for the action space
# normalization in the dataset class at data/pilot_dataset.py
# normalize: True

# # context
# context_type: temporal
# context_size: 5

# tradeoff between action and distance prediction loss
# alpha: 0.5
# beta: 10

# distance bounds for distance and action and distance predictions 
# distance:
#   min_dist_cat: 0
#   max_dist_cat: 20
# action:
#   min_dist_cat: 0
#   max_dist_cat: 10
# close_far_threshold: 10 # distance threshold used to seperate the close and the far subgoals that are sampled per datapoint

# # action output params
# len_traj_pred: 5
# learn_angle: True


# # dataset specific parameters
# image_size: [85, 64] # width, height
# goal_type: "image"

# use_wandb: True # set to false if you don't want to log to wandb
# train: True